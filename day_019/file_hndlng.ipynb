{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of  lines: 66 and Number of  words: 2377\n",
      "Number of  lines: 48 and Number of  words: 1259\n",
      "Number of  lines: 83 and Number of  words: 2203\n",
      "Number of  lines: 33 and Number of  words: 1375\n"
     ]
    }
   ],
   "source": [
    "# exercise num1\n",
    "\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "def count_words_lines(file):\n",
    "    '''Counts the number of lines and words in a file'''\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        words = []\n",
    "        for line in lines:\n",
    "            # removing all characters that are not alphanumeric using regex\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            words.extend(line.split())\n",
    "    print(f'Number of  lines: {len(lines)} and Number of  words: {len(words)}')\n",
    "\n",
    "\n",
    "count_words_lines('obama_speech.txt')\n",
    "count_words_lines('donald_speech.txt')\n",
    "count_words_lines('michelle_obama_speech.txt')\n",
    "count_words_lines('melania_trump_speech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise num2\n",
    "def most_spoken_languages(file_b,b):\n",
    "    '''\n",
    "    Takes a filepath and an integer,n as arguments and returns the n most spoken languages in the world\n",
    "    '''\n",
    "    with open(file_b) as fb:\n",
    "        list = json.loads(fb.read())\n",
    "    # looping to get dictionary \n",
    "    languages = []\n",
    "    for i in range(len(list)):\n",
    "        languages.extend(list[i]['languages'])\n",
    "    lang = {}\n",
    "    for language in languages:\n",
    "        lang[language] = lang.get(language,0) + 1\n",
    "    # sorting the list\n",
    "    sorted_lang = sorted(lang.items(), key= lambda x:x[1],reverse=True)\n",
    "    result = [(item[1],item[0]) for item in sorted_lang]\n",
    "    return result[:b]\n",
    "most_spoken_languages('countries_data.json',10)\n",
    "reslt = most_spoken_languages('countries_data.json',3)\n",
    "print('\\n', reslt,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 36101: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\AREWADS-30DAYS0F-PYTHON\\day_019\\file_hndlng.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     final_list \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m'\u001b[39m:item[\u001b[39m0\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mpopulation\u001b[39m\u001b[39m'\u001b[39m:item[\u001b[39m1\u001b[39m]} \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sorted_lt]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m final_list[:n]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m most_populated_countries(\u001b[39m'\u001b[39;49m\u001b[39mcountries_data.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\USER\\Documents\\GitHub\\AREWADS-30DAYS0F-PYTHON\\day_019\\file_hndlng.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mTakes a file path and an integer,n as arguments and returns the n most populated countries in the world\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     dic_list \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(f\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m population \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/GitHub/AREWADS-30DAYS0F-PYTHON/day_019/file_hndlng.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dic_list)):\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 36101: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Exercise num3\n",
    "def most_populated_countries(filename,n):\n",
    "    '''\n",
    "    Takes a file path and an integer,n as arguments and returns the n most populated countries in the world\n",
    "    '''\n",
    "    with open(filename) as f:\n",
    "        dic_list = json.loads(f.read())\n",
    "    population = dict()\n",
    "    for i in range(len(dic_list)):\n",
    "        keys = dic_list[i]['name']\n",
    "        values = dic_list[i]['population']\n",
    "        population[keys] = values\n",
    "    # sorting by values of the population\n",
    "    sorted_lt = sorted(population.items(), key= lambda x:x[1],reverse=True)\n",
    "    final_list = [{'country':item[0],'population':item[1]} for item in sorted_lt]\n",
    "    return final_list[:n]\n",
    "\n",
    "most_populated_countries('countries_data.json',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise num1\n",
    "with open('email_exchanges_big.txt') as f:\n",
    "    lines = f.readlines()\n",
    "email_addresses = []\n",
    "for line in lines:\n",
    "    #using regex to get alphanumeric characters before and after and @ symbol. \\S is non-whitespace characters\n",
    "    email_addresses.extend(re.findall('[a-zA-Z0-9]\\S*@\\S*[a-zA-Z]', line))\n",
    "email_addresses[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise num2\n",
    "def find_most_common_words(file,n=10):\n",
    "    '''\n",
    "    Returns a list of tuples showing the count of the n most common words in the document, file\n",
    "    '''\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        # removing all characters that are not whitespace or alphanumeric using regex\n",
    "        line = re.sub(r'[^\\w\\s]','',line)\n",
    "        words.extend(line.split())\n",
    "    words_dict = {}\n",
    "    for word in words:\n",
    "        words_dict[word] = words_dict.get(word,0) + 1\n",
    "    words_sorted = sorted(words_dict.items(),key=lambda x:x[1],reverse=True)\n",
    "    result = [(word[1],word[0]) for word in words_sorted]\n",
    "    return result[:n]\n",
    "\n",
    "find_most_common_words('donald_speech.txt',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise num3\n",
    "# The ten most frequent words used in Obama's speech\n",
    "print('Obama: ',find_most_common_words('obama_speech.txt')) \n",
    "# The ten most frequent words used in Michelle's speech \n",
    "print('Michelle Obama: ',find_most_common_words('michelle_obama_speech.txt'))\n",
    "# The ten most frequent words used in Trump's speech \n",
    "print('Donald Trump: ',find_most_common_words('donald_speech.txt'))\n",
    "# The ten most frequent words used in Melina's speech\n",
    "print('Melania Trump: ',find_most_common_words('melania_trump_speech.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise num4\n",
    "def clean_text(file):\n",
    "    '''\n",
    "    Returns all the words in a text after removing the punctuations and others\n",
    "    '''\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        words = []\n",
    "        for line in lines:\n",
    "            # removing all characters that are not whitespace or alphanumeric using regex\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            words.extend(line.split())\n",
    "    return words\n",
    "clean_text('michelle_obama_speech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stop_words list from stop_words.py\n",
    "from stop_words import stop_words\n",
    "\n",
    "def remove_stop_words(list):\n",
    "    return [word for word in list if word.lower() not in stop_words]\n",
    "remove_stop_words(clean_text('michelle_obama_speech.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_similarity(list_one,list_two):\n",
    "    res = [x for x in (list_one + list_two) if x in list_one and x in list_two]\n",
    "    similar_words_percent = (len(res)/(len(list_one) + len(list_two))) * 100\n",
    "    return similar_words_percent\n",
    "check_text_similarity(['apple','banana','mango','pawpaw'],['apple','mango','pear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the function for a big one: biggest I have tried so far:\n",
    "def comparing_text_in_file_similarity(file_one,file_two):\n",
    "    file_one_words = remove_stop_words(clean_text(file_one))\n",
    "    file_two_words = remove_stop_words(clean_text(file_two))\n",
    "    return check_text_similarity(file_one_words,file_two_words)\n",
    "\n",
    "comparing_text_in_file_similarity('michelle_obama_speech.txt','melania_trump_speech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise num5\n",
    "print('10 most common words in romeo and juliet: ',find_most_common_words('romeo_and_juliet.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hacker_news.csv',newline='') as f:\n",
    "    csv_reader = csv.reader(f,delimiter=',')\n",
    "    python_rows = 0\n",
    "    javascript_rows = 0\n",
    "    java_rows = 0\n",
    "    for row in csv_reader:\n",
    "        for i in range(len(row)):\n",
    "            if re.findall(r'[Pp]ython',row[i]):\n",
    "                python_rows += 1\n",
    "            elif re.findall(r'[Jj]ava[Ss]cript',row[i]):\n",
    "                javascript_rows +=1\n",
    "            elif re.findall(r'Java$',row[i]):\n",
    "                java_rows +=1\n",
    "print(f'the number of lines containing a,b and c respectively are: {python_rows}, {javascript_rows} and {java_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "with open('donald_speech.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        # removing all characters that are not whitespace or alphanumeric using regex, don't want punctuation affecting count\n",
    "        line = re.sub(r'[^\\w\\s]','',line)\n",
    "        words.extend(line.split())\n",
    "Count = Counter(words)\n",
    "Count.most_common(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e880a7f5f27ad91acd9cec33fbf0d93d234c7ab08dff5f8487ebbcd75c6e380"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
